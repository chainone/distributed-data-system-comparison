## Preface
While I was reading the book "Designing Data-Intensive Applications "", I was trying to understand the essence of distributed data systems and what are the techniques commonly applied in those data systems that we (as developers) interact with everyday and what are the pros and cons.

Though the book mentioned some of those data systems throughout the book, there is no central place to categorize all the popular existing distributed data systems by different characteristics.
That's why I have the idea to make a such table so that it could help me to memorize and also it may be valuable to all the developers that are interested in this area.

CAUTION: the table is not finished yet nor 100% correct, contributions are welcomed!

## Detailed comparison table

[cols="10*", options="header"]
|===

2+| Data Systems | MySQL | HBase | Cassandra | DynamoDB | MongoDB | ElasticSearch  | Kafka | RabbitMQ

.7+s| Replication

s| Write
| Single write to leader
a|[.small]
Single write to the target region server
First write needs lookup (location of meta table) in zk first and then the region server that holds meta table and then do the actual write
meta table could be cached in client

a|[.small]
Multi-write, select any node as coordinator node and forward request to the nodes hold the target data
| ?
a|[.small]
Single write to the primary node of the target Replica Set  and then async to secondaries,
reads from secondaries may return data that does not reflect the state of the data on the primary
a|[.small]
Write to one primary shard(partition) first and then sync changed documents to replica shards asynchronously (? some docs saying otherwise)

| Y
| ?



s| Replication Type
a|[.small]
Single leader, all data replication
a|[.small]
Partition based on key range, write to single region server, replica via HDFS

a|[.small]
Leaderless, partition, multi-writes to selected nodes and with read repair if stable value is obtained

| Leaderless
a|[.small]
leader-based in one replica set, all data on primary need to be synced to secondaries
a|[.small]
leader-based (primary shard vs replica shards)
| leader-based
| leader-based



s| Sync/Async Replication
a|[.small]
Configurable, default semi-sync
a|[.small]
Kinda SYNC, Write to WAL(persistent in HDFS) and in member store and then return
a|[.small]
Configurable, the w in quorum ((w + r > n)
| ?
| Async
| Async(? some docs say otherwise)
| Y
| Y




s| Failure Over
a|[.small]

* Follower: Follower copies latest snapshot from leader and then catch up with binlog
* Leader: Followers will most recent binlog becomes leader, old leader's unreplicated writes are simply discarded

a|[.small]
* ZK: whole cluster is down
* Master: need in-active master as hot backup, otherwise DDL operations will fail
* Region Servers: node recover from WAL, while recovering, that part of data is not available

a|[.small]
NO IMPACT as long as quorum ((w + r > n)) is satisfied

a|[.small]
Should be same as Cassandra
a|[.small]
* Primary: eligible secondary calls for an election to nominate itself as the new primary. The replica set cannot process write operations until the election completes successfully.
The replica set can continue to serve read queries if such queries are configured to run on secondaries while the primary is offline.
* Secondary: first copy all data from one of the members, and then apply all changes by syncing oplog

|
* Master: HA masters, if no master, elect one other node as master.
* Data nodes: if the failure node contains primary shards, one of the replica shards will be promoted (by master) to be primary, otherwise writing to that shard is not allowed

| Y
| ?





s| Multi-leader Replication Topology
a|[.small]
Circular by default (Cluster version)
| NA
a|[.small]
Circular
| ?
a|[.small]
NA, Secondary (Follower) chooses to sync oplog from Primary or Secondary based on ping time and the state of other secondary's replica status
a|[.small]
NA, Leader(node with that primary shard) forwards changed documents to nodes with replicas

| Y
| Y



s| Replication Logs
a|[.small]
Originally STATEMENT-BASED, default to LOGICAL(row-based) if any nondeterminism in statement
| WAL
a|[.small]
Commit Log, just like the WAL in HBase, however, the write doesn't wait for finishing writing to in-memory store
| Y
a|[.small]
Op Log, should be STATEMENT-BASED with transforms
| No Log, copy shards initially, and then forward changed documents to sync between primary shard and replicas
| Y
| Y



s| Multi-Write Conflict Resolve
a|[.small]
NA (as all writes are sent to leader)
a|[.small]
NA (as writes are region-based, no conflict)
a|[.small]
LWW (last write win)
| Y
a|[.small]
NA (as write are shard(partition) based, no conflict)
a|[.small]
NA (as write are shard(partition) based, no conflict)

| Y
| Y




.4+s| Partition
s| Partitioning Strategy
| NA
| Key Range
a|[.small]
First Key Hash, left Key Range
| ?
a|[.small]
Key range before 2.4, hash and range key both support later on
| Key Hash
| Y
| ?



s| Secondary Indexes
| NA
| No secondary index by default
| Local
| Global(term-partitioned)
| Local
| Local
| ?
| ?




s| Rebalancing Strategy
| NA
| Dynamic Partitioning
a|[.small]
Partitioning proportionally to nodes, move split partitions between
| ?
a|[.small]
Number of partitions equals to that of replica sets, one partition has a lot of 64MB-size chunks,
partitions could be added later one and the number of chunks will be re-balanced across partitions (shards)

a|[.small]
Fixed number of partitions per index, entire partitions moved between nodes

| Kafka
| RabbitMQ






s| Request Routing
| NA
a|[.small]
Routing Tier(ZK), if no cache on client, meta table looking-up in zk first and then the region server is required
meta table could be cached in client
a|[.small]
Client request to any node and then forward if miss
| DynamoDB
a|[.small]
Routing Tier (multiple mongos to route and aggregate, and one config server to store data location information(on which partition))
a|[.small]
Routing Tier (client node)
a|[.small]
? Routing Tier(or partition aware client?)(ZK)
| RabbitMQ
|===

## CAP

[cols="8*", options="header"]
|===
| MySQL | HBase | Cassandra | DynamoDB | MongoDB | ElasticSearch  | Kafka | RabbitMQ

| P, Not C nor A
| CP
| AP, Eventually C
| AP, Eventually C
| P, Not A (during failure-over election), Not C (as async replica sync)
| P, Not A (during the promotion of primary shards), Not C (as async replica sync)
| Kafka
| RabbitMQ

|===

## Reference

. Designing Data-Intensive Applications (link:https://dataintensive.net/[])
. MongoDB: The Definitive Guide, 2nd Edition (link:http://shop.oreilly.com/product/0636920028031.do[])
. The MongoDB 4.0 Manual (link:https://docs.mongodb.com/manual/[])
. Elasticsearch: The Definitive Guide (link:https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html[])
. Elasticsearch Reference (link:https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[])
. Cassandra: The Definitive Guide (link:http://shop.oreilly.com/product/0636920010852.do[])
. Kafka: The Definitive Guide (link:http://shop.oreilly.com/product/0636920044123.do[])
. RabbitMQ in Action (link:https://www.manning.com/books/rabbitmq-in-action[])
