## Preface
While I was reading the book "Designing Data-Intensive Applications", I was trying to understand the essence of distributed data systems and what are the techniques commonly applied in those data systems that we (as developers) interact with everyday and what are the pros and cons.

Though the book mentioned some of those data systems throughout the book, there is no central place to categorize all the popular existing distributed data systems by different characteristics.
That's why I have the idea to make a such table so that it could help me to memorize and also it may be valuable to all the developers that are interested in this area.

CAUTION: The comparison table is not finished yet nor 100% correct, contributions are welcomed!

## Detailed comparison table

[cols="10*", options="header"]
|===

2+| Data Systems | MySQL(Ma-Sl) | HBase | Cassandra | DynamoDB | MongoDB | ElasticSearch  | Kafka | RabbitMQ

.7+s| Replication

s| Write
| Single write to leader
a|[.small]
Single write to the target region server
First write needs lookup (location of meta table) in zk first and then the region server that holds meta table and then do the actual write
meta table could be cached in client

a|[.small]
Multi-write, select any node as coordinator node and forward request to the nodes hold the target data
| ?
a|[.small]
Single write to the primary node of the target Replica Set  and then async to secondaries,
reads from secondaries may return data that does not reflect the state of the data on the primary
a|[.small]
Write to one primary shard(partition) first and then sync changed documents to replica shards asynchronously (? some docs saying otherwise)

a|[.small]
The producer(client) uses the parameter "ack" to control how many partition replicas must receive the record before the producer can consider the write successful.

| ?



s| Replication Type
a|[.small]
Single leader, all data replication
a|[.small]
Partition based on key range, write to single region server, replica via HDFS
a|[.small]
Leaderless, partition, multi-writes to selected nodes and with read repair if stable value is obtained
| Leaderless
a|[.small]
leader-based in one replica set, all data on primary need to be synced to secondaries
a|[.small]
leader-based (primary shard vs replica shards)
a|[.small]
leader-based (leader replica vs follower replica)

CAUTION: Replicas don't serve any requests, it is merely for backup

a|[.small]
leader-based


s| Sync/Async Replication
a|[.small]
Configurable, default semi-sync
a|[.small]
Kinda SYNC, Write to WAL(persistent in HDFS) and in member store and then return
a|[.small]
Configurable, the w in quorum ((w + r > n)
| ?
| Async
| Async(? some docs say otherwise)
a|[.small]
Sync, produced messages are considered “committed” when they were written to the partition on all its in-sync replicas .
However, producer(client) can control how many partition replicas must receive the record before the producer can consider the write successful.
| Y




s| Failure Over
a|[.small]

* Follower: Follower copies latest snapshot from leader and then catch up with binlog
* Leader: Followers will most recent binlog becomes leader, old leader's unreplicated writes are simply discarded

a|[.small]
* ZK: whole cluster is down
* Master: need in-active master as hot backup, otherwise DDL operations will fail
* Region Servers: node recover from WAL, while recovering, that part of data is not available

a|[.small]
NO IMPACT as long as quorum ((w + r > n)) is satisfied
a|[.small]
Should be same as Cassandra
a|[.small]
* Primary: eligible secondary calls for an election to nominate itself as the new primary. The replica set cannot process write operations until the election completes successfully.
The replica set can continue to serve read queries if such queries are configured to run on secondaries while the primary is offline.
* Secondary: first copy all data from one of the members, and then apply all changes by syncing oplog
a|[.small]
* Master: HA masters, if no master, elect one other node as master.
* Data nodes: if the failure node contains primary shards, one of the replica shards will be promoted (by master) to be primary, otherwise writing to that shard is not allowed
a|[.small]
* Controller: When the controller broker is stopped or loses connectivity to Zookeeper, the ephemeral node will disappear. Other brokers in the cluster will be notified through the Zookeeper watch that the controller is gone and will attempt to create the controller node in Zookeeper themselves. The first node to create the new controller in Zoo‐ keeper is the new controller.
* Other brokers: for those leader replicas on this node, one of the corresponding in-sync replicas will be elected to be the leader replica. If there is no in-sync replica available, we can use "unclean.leader.election.enable" to control if we allow out-of-sync replica to be the new leader,In summary, if we allow out-of-sync replicas to become leaders, we risk data loss and data inconsistencies. If we don’t allow them to become leaders, we face lower availability.

| ?





s| Multi-leader Replication Topology
a|[.small]
Circular by default (Cluster version)
| NA
a|[.small]
Circular
| ?
a|[.small]
NA, Secondary (Follower) chooses to sync oplog from Primary or Secondary based on ping time and the state of other secondary's replica status
a|[.small]
NA, Leader(node with that primary shard) forwards changed documents to nodes with replicas

| NA
| Y



s| Replication Logs
a|[.small]
Originally STATEMENT-BASED, default to LOGICAL(row-based) if any nondeterminism in statement
| WAL
a|[.small]
Commit Log, just like the WAL in HBase, however, the write doesn't wait for finishing writing to in-memory store
| Y
a|[.small]
Op Log, should be STATEMENT-BASED with transforms
a|[.small]
No Log, copy shards initially, and then forward changed documents to sync between primary shard and replicas
a|[.small]
The topic is actually a partition-ed log, brokers having follower replicas receive messages from other brokers having corresponding leader replicas using log offset just like how the client consumes messages.
| Y


s| Multi-Write Conflict Resolve
a|[.small]
NA (as all writes are sent to leader)
a|[.small]
NA (as writes are region-based, no conflict)
a|[.small]
LWW (last write win)
| Y
a|[.small]
NA (as write are shard(partition) based, no conflict)
a|[.small]
NA (as write are shard(partition) based, no conflict)
a|[.small]
NA (as write are shard(partition) based, no conflict)
| Y


.4+s| Partition
s| Partitioning Strategy
| NA
| Key Range
a|[.small]
First Key Hash, left Key Range
| ?
a|[.small]
Key range before 2.4, hash and range key both support later on
| Key Hash
a|[.small]
Decided on the producer side in the topic bias, producer can choose to do hash-based partitioning(which is default) or implement its own partiton strategy. Once it is chosen, it can not be changed (which means the partition number is fixed no matter how many nodes you add later on).
| ?


s| Secondary Indexes
| NA
| No secondary index by default
| Local
| Global(term-partitioned)
| Local
| Local
| NA
| ?




s| Rebalancing Strategy
| NA
| Dynamic Partitioning
a|[.small]
Partitioning proportionally to nodes, move split partitions between
| ?
a|[.small]
Number of partitions equals to that of replica sets, one partition has a lot of 64MB-size chunks,
partitions could be added later one and the number of chunks will be re-balanced across partitions (shards)
a|[.small]
Fixed number of partitions per index, entire partitions moved between nodes
a|[.small]
Manual, need to use tool "kafka-reassign-partitions.sh" to do the partition rebalance. On the new broker, new replicas are created first and then old replicas are removed.
Removing many partitions from a single broker, such as if that broker is being removed from the cluster, it is a best practice to shut down and restart the broker before starting the reassignment. This will move leadership for the partitions on that particular broker to other brokers in the cluster. This can significantly increase the per‐ formance of reassignments and reduce the impact on the cluster as the replication traffic will be distributed to many brokers.
| RabbitMQ






s| Request Routing
| NA
a|[.small]
Routing Tier(ZK), if no cache on client, meta table looking-up in zk first and then the region server is required
meta table could be cached in client
a|[.small]
Client request to any node and then forward if miss
| DynamoDB
a|[.small]
Routing Tier (multiple mongos to route and aggregate, and one config server to store data location information(on which partition))
a|[.small]
Routing Tier (node with client role)
a|[.small]
Partition aware client
Producer knows which broker to sent partitioned message to, and consumer knows which partitions he is responsible for receiving the messages from
| RabbitMQ
|===

## CAP

[cols="8*", options="header"]
|===
| MySQL | HBase | Cassandra | DynamoDB | MongoDB | ElasticSearch  | Kafka | RabbitMQ

| P, Not C nor A
| CP
| AP, Eventually C
| AP, Eventually C
| P, Not A (during failure-over election), Not C (as async replica sync)
| P, Not A (during the promotion of primary shards), Not C (as async replica sync)
| Kafka
| RabbitMQ

|===


## TODO
. Add more data systems: zookeeper, etcd, consule
. Clarification on CAP
. Add "Read behavior","Dependencies", "Consensus Algorithm", "Distributed Transaction" in the table

## Reference

. Designing Data-Intensive Applications (link:https://dataintensive.net/[])
. MongoDB: The Definitive Guide, 2nd Edition (link:http://shop.oreilly.com/product/0636920028031.do[])
. The MongoDB 4.0 Manual (link:https://docs.mongodb.com/manual/[])
. Elasticsearch: The Definitive Guide (link:https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html[])
. Elasticsearch Reference (link:https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[])
. Cassandra: The Definitive Guide (link:http://shop.oreilly.com/product/0636920010852.do[])
. Kafka: The Definitive Guide (link:http://shop.oreilly.com/product/0636920044123.do[])
. RabbitMQ in Action (link:https://www.manning.com/books/rabbitmq-in-action[])
